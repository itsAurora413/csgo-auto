#!/usr/bin/env python3
"""
PoC: Prophet + XGBoost é›†æˆæ¨¡å‹ vs çº¿æ€§å›å½’å¯¹æ¯”
- æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘å¤„ç†
- é›†æˆç¼“å­˜ç³»ç»Ÿï¼ŒåŠ é€Ÿé‡å¤æŸ¥è¯¢
- ä¼˜åŒ–å¤§æ•°æ®é‡å¤„ç†
"""

import sys
import json
import warnings
import pickle
from datetime import datetime, timedelta
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error
from prophet import Prophet
from xgboost import XGBRegressor
import pymysql
import os

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ç¼“å­˜é…ç½®
CACHE_DIR = Path('/Users/user/Downloads/csgoAuto/.cache')
CACHE_DIR.mkdir(exist_ok=True)


class Cache:
    """ç®€å•çš„æ–‡ä»¶ç¼“å­˜ç³»ç»Ÿï¼Œæ”¯æŒå¹¶å‘è®¿é—®"""
    def __init__(self, cache_dir=CACHE_DIR):
        self.cache_dir = cache_dir
        self.lock = Lock()

    def get(self, key):
        """è·å–ç¼“å­˜"""
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"  âš ï¸  ç¼“å­˜è¯»å–å¤±è´¥: {e}")
                return None
        return None

    def set(self, key, value):
        """è®¾ç½®ç¼“å­˜"""
        with self.lock:
            cache_file = self.cache_dir / f"{key}.pkl"
            try:
                with open(cache_file, 'wb') as f:
                    pickle.dump(value, f)
            except Exception as e:
                print(f"  âš ï¸  ç¼“å­˜å†™å…¥å¤±è´¥: {e}")

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            for f in self.cache_dir.glob('*.pkl'):
                f.unlink()


class DatabasePool:
    """æ•°æ®åº“è¿æ¥æ± ï¼Œç”¨äºå¤šçº¿ç¨‹å¹¶å‘è®¿é—®"""
    def __init__(self, db_host='23.254.215.66', db_user='root', db_password='Wyj250413.',
                 db_name='csgo_trader', pool_size=10):
        self.db_config = {
            'host': db_host,
            'user': db_user,
            'password': db_password,
            'database': db_name,
            'charset': 'utf8mb4'
        }
        self.pool_size = pool_size
        self.connections = []
        self.lock = Lock()
        self._init_pool()

    def _init_pool(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        for _ in range(self.pool_size):
            try:
                conn = pymysql.connect(**self.db_config)
                self.connections.append(conn)
            except Exception as e:
                print(f"âŒ è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}")

    def get_connection(self):
        """è·å–è¿æ¥"""
        with self.lock:
            if self.connections:
                return self.connections.pop()
            return pymysql.connect(**self.db_config)

    def release_connection(self, conn):
        """é‡Šæ”¾è¿æ¥"""
        with self.lock:
            if len(self.connections) < self.pool_size:
                self.connections.append(conn)
            else:
                conn.close()

    def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        with self.lock:
            for conn in self.connections:
                try:
                    conn.close()
                except:
                    pass
            self.connections.clear()


class EnsembleModelPOC:
    """Prophet + XGBoost é›†æˆæ¨¡å‹ PoC"""

    def __init__(self, db_host='23.254.215.66', db_user='root', db_password='Wyj250413.',
                 db_name='csgo_trader', num_workers=8):
        """åˆå§‹åŒ–"""
        self.db_pool = DatabasePool(db_host, db_user, db_password, db_name, pool_size=num_workers)
        self.cache = Cache()
        self.num_workers = num_workers
        self.results_lock = Lock()
        self.all_results = []

        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ: {num_workers} ä¸ªå·¥ä½œçº¿ç¨‹, ç¼“å­˜ç›®å½•: {CACHE_DIR}")

    def fetch_historical_data(self, good_id, days=30, use_cache=True):
        """ä»æ•°æ®åº“è·å–å†å²ä»·æ ¼æ•°æ®ï¼Œæ”¯æŒç¼“å­˜"""
        cache_key = f"hist_data_{good_id}_{days}"

        # å°è¯•è¯»å–ç¼“å­˜
        if use_cache:
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                return cached_data

        conn = self.db_pool.get_connection()
        try:
            cursor = conn.cursor()
            query = """
            SELECT created_at, yyyp_buy_price, yyyp_sell_price,
                   yyyp_buy_count, yyyp_sell_count
            FROM csqaq_good_snapshots
            WHERE good_id = %s
            AND created_at >= DATE_SUB(NOW(), INTERVAL %s DAY)
            AND yyyp_buy_price > 0 AND yyyp_sell_price > 0
            ORDER BY created_at ASC
            """
            cursor.execute(query, (good_id, days))
            results = cursor.fetchall()

            if not results:
                return None

            df = pd.DataFrame(results, columns=[
                'timestamp', 'buy_price', 'sell_price',
                'buy_orders', 'sell_orders'
            ])

            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp').reset_index(drop=True)

            # ç¼“å­˜æ•°æ®
            if use_cache:
                self.cache.set(cache_key, df)

            return df
        finally:
            cursor.close()
            self.db_pool.release_connection(conn)

    def get_sample_templates_concurrent(self, limit=5):
        """å¹¶å‘è·å–æ ·æœ¬æ¨¡æ¿ - ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–"""
        conn = self.db_pool.get_connection()
        try:
            cursor = conn.cursor()
            query = """
            SELECT good_id, COUNT(*) as data_points
            FROM csqaq_good_snapshots
            WHERE created_at >= DATE_SUB(NOW(), INTERVAL 30 DAY)
            AND yyyp_buy_price > 0 AND yyyp_sell_price > 0
            GROUP BY good_id
            HAVING data_points >= 20
            ORDER BY data_points DESC
            LIMIT %s
            """
            cursor.execute(query, (limit,))
            samples = cursor.fetchall()
            return samples
        finally:
            cursor.close()
            self.db_pool.release_connection(conn)

    def prepare_features(self, df):
        """ä¸ºXGBoostå‡†å¤‡ç‰¹å¾"""
        df_features = df.copy()

        # æ—¶é—´ç‰¹å¾
        df_features['day_of_week'] = df_features['timestamp'].dt.dayofweek
        df_features['day_of_month'] = df_features['timestamp'].dt.day
        df_features['days_since_start'] = (df_features['timestamp'] - df_features['timestamp'].min()).dt.days

        # ä»·æ ¼ç‰¹å¾
        df_features['price_range'] = df_features['sell_price'] - df_features['buy_price']
        df_features['total_orders'] = df_features['buy_orders'] + df_features['sell_orders']
        df_features['order_ratio'] = df_features['buy_orders'] / (df_features['sell_orders'] + 1)

        # ç§»åŠ¨å¹³å‡
        df_features['buy_price_ma3'] = df_features['buy_price'].rolling(3, min_periods=1).mean()
        df_features['sell_price_ma3'] = df_features['sell_price'].rolling(3, min_periods=1).mean()

        # å¤„ç†ç¼ºå¤±å€¼
        df_features = df_features.fillna(method='ffill').fillna(method='bfill')

        return df_features

    def train_and_evaluate(self, good_id, idx, total):
        """è®­ç»ƒå’Œè¯„ä¼°å•ä¸ªå•†å“çš„æ¨¡å‹"""
        try:
            print(f"  [{idx}/{total}] å¤„ç† Good ID {good_id}...", flush=True)

            # è·å–å†å²æ•°æ®
            df = self.fetch_historical_data(good_id, days=30)

            if df is None or len(df) < 10:
                print(f"  [{idx}/{total}] âš ï¸  æ•°æ®ä¸è¶³ï¼Œè·³è¿‡", flush=True)
                return None

            # æ•°æ®åˆ†å‰² (70% è®­ç»ƒ, 30% æµ‹è¯•)
            split_point = int(len(df) * 0.7)
            df_train = df[:split_point].copy()
            df_test = df[split_point:].copy()

            # ========== çº¿æ€§å›å½’ ==========
            X_train_len = len(df_train)
            y_train = df_train['sell_price'].values
            lr_model = LinearRegression()
            lr_model.fit(np.arange(len(y_train)).reshape(-1, 1), y_train)
            X_test_lr = np.arange(X_train_len, X_train_len + len(df_test)).reshape(-1, 1)
            y_pred_lr = lr_model.predict(X_test_lr)

            # ========== Prophet ==========
            df_prophet = df_train[['timestamp', 'sell_price']].copy()
            df_prophet.columns = ['ds', 'y']
            prophet_model = Prophet(yearly_seasonality=False, interval_width=0.95)
            prophet_model.fit(df_prophet)
            future = prophet_model.make_future_dataframe(periods=len(df_test))
            forecast = prophet_model.predict(future)
            y_pred_prophet = forecast['yhat'].values[-len(df_test):]

            # ========== XGBoost ==========
            df_features = self.prepare_features(df_train)
            feature_cols = ['day_of_week', 'day_of_month', 'days_since_start',
                           'price_range', 'total_orders', 'order_ratio',
                           'buy_price_ma3', 'sell_price_ma3']
            X_train_xgb = df_features[feature_cols].values
            y_train_xgb = df_features['sell_price'].values

            xgb_model = XGBRegressor(n_estimators=50, max_depth=4, learning_rate=0.1,
                                    random_state=42, verbosity=0)
            xgb_model.fit(X_train_xgb, y_train_xgb)

            # å‡†å¤‡æµ‹è¯•ç‰¹å¾
            df_test_features = self.prepare_features(pd.concat([df_train, df_test], ignore_index=True))
            df_test_features = df_test_features[len(df_train):].reset_index(drop=True)
            X_test_xgb = df_test_features[feature_cols].values
            y_pred_xgb = xgb_model.predict(X_test_xgb)

            # ========== è¯„ä¼° ==========
            y_test = df_test['sell_price'].values
            y_pred_lr = np.maximum(y_pred_lr, 0)
            y_pred_prophet = np.maximum(y_pred_prophet, 0)
            y_pred_xgb = np.maximum(y_pred_xgb, 0)

            def calc_metrics(y_true, y_pred):
                mape = mean_absolute_percentage_error(y_true, y_pred) * 100
                rmse = np.sqrt(mean_squared_error(y_true, y_pred))
                mae = mean_absolute_error(y_true, y_pred)
                return {'MAPE': mape, 'RMSE': rmse, 'MAE': mae}

            metrics = {
                'çº¿æ€§å›å½’': calc_metrics(y_test, y_pred_lr),
                'Prophet': calc_metrics(y_test, y_pred_prophet),
                'XGBoost': calc_metrics(y_test, y_pred_xgb)
            }

            # é›†æˆæ¨¡å‹ (åŠ æƒå¹³å‡)
            y_pred_ensemble = (y_pred_lr * 0.2 + y_pred_prophet * 0.3 + y_pred_xgb * 0.5)
            metrics['é›†æˆæ¨¡å‹'] = calc_metrics(y_test, y_pred_ensemble)

            print(f"  [{idx}/{total}] âœ“ å®Œæˆ (MAPE: LR={metrics['çº¿æ€§å›å½’']['MAPE']:.1f}% Prophet={metrics['Prophet']['MAPE']:.1f}% XGB={metrics['XGBoost']['MAPE']:.1f}% Ensemble={metrics['é›†æˆæ¨¡å‹']['MAPE']:.1f}%)", flush=True)

            return {
                'good_id': good_id,
                'data_points': len(df),
                'train_size': len(df_train),
                'test_size': len(df_test),
                'metrics': metrics
            }

        except Exception as e:
            print(f"  [{idx}/{total}] âŒ é”™è¯¯: {str(e)}", flush=True)
            return None

    def run_poc(self):
        """è¿è¡Œå®Œæ•´çš„ PoC"""
        print("\n" + "="*70)
        print("Prophet + XGBoost é›†æˆæ¨¡å‹ PoC (å¤šçº¿ç¨‹ä¼˜åŒ–ç‰ˆ)")
        print("="*70)

        print("\n1ï¸âƒ£  æ­£åœ¨è·å–æ ·æœ¬å•†å“æ•°æ®...")
        start_time = time.time()
        samples = self.get_sample_templates_concurrent(limit=5)
        elapsed = time.time() - start_time

        if not samples:
            print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®")
            return

        print(f"âœ“ è·å– {len(samples)} ä¸ªæ ·æœ¬ (è€—æ—¶: {elapsed:.2f}s)")

        # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘å¤„ç†
        print(f"\n2ï¸âƒ£  å¯åŠ¨ {self.num_workers} ä¸ªå·¥ä½œçº¿ç¨‹å¤„ç†æ•°æ®...")
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            futures = [
                executor.submit(self.train_and_evaluate, good_id, idx + 1, len(samples))
                for idx, (good_id, _) in enumerate(samples)
            ]

            for future in as_completed(futures):
                result = future.result()
                if result is not None:
                    with self.results_lock:
                        self.all_results.append(result)

        elapsed = time.time() - start_time
        print(f"\nâœ“ å¹¶å‘å¤„ç†å®Œæˆ (è€—æ—¶: {elapsed:.2f}s, æˆåŠŸ: {len(self.all_results)}/{len(samples)})")

        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        if self.all_results:
            self.generate_report(self.all_results)

        self.db_pool.close_all()
        print("\nâœ“ æ‰€æœ‰è¿æ¥å·²å…³é—­")

    def generate_report(self, results):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        print("\n" + "="*70)
        print("ğŸ“ˆ æ€»ä½“æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print("="*70)

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        model_names = ['çº¿æ€§å›å½’', 'Prophet', 'XGBoost', 'é›†æˆæ¨¡å‹']
        summary = {m: {'MAPE': [], 'RMSE': [], 'MAE': []} for m in model_names}

        for result in results:
            for model_name in model_names:
                if model_name in result['metrics']:
                    summary[model_name]['MAPE'].append(result['metrics'][model_name]['MAPE'])
                    summary[model_name]['RMSE'].append(result['metrics'][model_name]['RMSE'])
                    summary[model_name]['MAE'].append(result['metrics'][model_name]['MAE'])

        print(f"\n{'æ¨¡å‹':<12} {'å¹³å‡MAPE':<12} {'å¹³å‡RMSE':<12} {'å¹³å‡MAE':<12}")
        print("-" * 48)

        for model_name in model_names:
            if summary[model_name]['MAPE']:
                avg_mape = np.mean(summary[model_name]['MAPE'])
                avg_rmse = np.mean(summary[model_name]['RMSE'])
                avg_mae = np.mean(summary[model_name]['MAE'])
                print(f"{model_name:<12} {avg_mape:>10.2f}%  {avg_rmse:>10.2f}  {avg_mae:>10.2f}")

        # ä¿å­˜è¯¦ç»†ç»“æœä¸ºJSON
        output_file = '/Users/user/Downloads/csgoAuto/poc_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'summary': {k: {m: float(np.mean(v)) if v else 0 for m, v in mv.items()}
                           for k, mv in summary.items()},
                'detailed_results': results
            }, f, indent=2, ensure_ascii=False)

        print(f"\nâœ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

        # å…³é”®å‘ç°
        print("\n" + "="*70)
        print("ğŸ” å…³é”®å‘ç°")
        print("="*70)

        avg_mape = {
            'çº¿æ€§å›å½’': np.mean(summary['çº¿æ€§å›å½’']['MAPE']) if summary['çº¿æ€§å›å½’']['MAPE'] else 0,
            'Prophet': np.mean(summary['Prophet']['MAPE']) if summary['Prophet']['MAPE'] else 0,
            'XGBoost': np.mean(summary['XGBoost']['MAPE']) if summary['XGBoost']['MAPE'] else 0,
            'é›†æˆæ¨¡å‹': np.mean(summary['é›†æˆæ¨¡å‹']['MAPE']) if summary['é›†æˆæ¨¡å‹']['MAPE'] else 0,
        }

        best_model = min(avg_mape.items(), key=lambda x: x[1])
        improvement_vs_lr = ((avg_mape['çº¿æ€§å›å½’'] - best_model[1]) / avg_mape['çº¿æ€§å›å½’'] * 100) if avg_mape['çº¿æ€§å›å½’'] > 0 else 0

        print(f"\n1. æœ€ä½³æ¨¡å‹: {best_model[0]} (å¹³å‡MAPE: {best_model[1]:.2f}%)")
        print(f"2. ç›¸å¯¹çº¿æ€§å›å½’çš„æ”¹è¿›: {improvement_vs_lr:.1f}%")
        print(f"3. å¤„ç†çš„å•†å“æ•°: {len(self.all_results)}")
        print(f"\n4. é›†æˆæ¨¡å‹çš„ä¼˜åŠ¿:")
        print(f"   - æ•´åˆ Prophet çš„è¶‹åŠ¿+å­£èŠ‚æ€§èƒ½åŠ›")
        print(f"   - æ•´åˆ XGBoost çš„éçº¿æ€§å…³ç³»å­¦ä¹ ")
        print(f"   - åŠ æƒèåˆé¿å…è¿‡æ‹Ÿåˆ")
        print(f"   - MAPE æ”¹è¿›: {improvement_vs_lr:.1f}%")

        print(f"\n5. ç¼“å­˜æ•ˆæœ:")
        cache_size = sum(f.stat().st_size for f in CACHE_DIR.glob('*.pkl')) / 1024 / 1024
        print(f"   - ç¼“å­˜å¤§å°: {cache_size:.2f} MB")
        print(f"   - ç¼“å­˜ä½ç½®: {CACHE_DIR}")
        print(f"   - æç¤º: é‡æ–°è¿è¡Œ PoC ä¼šä½¿ç”¨ç¼“å­˜ï¼Œé€Ÿåº¦ä¼šæ›´å¿«")

        print("\nâœ… PoC éªŒè¯å®Œæˆï¼")


if __name__ == '__main__':
    poc = EnsembleModelPOC(num_workers=8)
    poc.run_poc()
