#!/usr/bin/env python3
"""
CSGO é¥°å“å¸‚åœº - é«˜çº§é¢„æµ‹æœåŠ¡ v2.1 (æ€§èƒ½ä¼˜åŒ–ç‰ˆ)
ç‰¹æ€§:
- æ¨¡å‹æŒä¹…åŒ–ï¼ˆpickleä¿å­˜ï¼‰
- å¢é‡è®­ç»ƒï¼ˆåŠ è½½æ—§æ¨¡å‹ç»§ç»­è®­ç»ƒï¼‰
- è´¨é‡æŒ‡æ ‡è·Ÿè¸ªï¼ˆMSEã€MAPEã€MAEç­‰ï¼‰
- è®­ç»ƒå†å²è®°å½•
- é«˜å¹¶å‘ä¼˜åŒ– (ç»†ç²’åº¦é”, è¿æ¥æ± , çº¿ç¨‹æ± )
- åŠ¨æ€æƒé‡åˆ†é…
"""

import sys
import json
import warnings
import pickle
import logging
import queue
from datetime import datetime, timedelta
from pathlib import Path
from threading import Lock
from dataclasses import asdict
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error
from prophet import Prophet
from xgboost import XGBRegressor
import pymysql
from flask import Flask, jsonify, request
from flask_cors import CORS

# å¯¼å…¥æ•°æ®è´¨é‡å’Œæ¼‚ç§»æ£€æµ‹æ¨¡å—
from data_quality_monitor import DataQualityChecker, DataDriftDetector, DataCleaner
from drift_alert_system import DriftAlertSystem, RetrainingTrigger

warnings.filterwarnings('ignore')

# ============================================================================
# é…ç½®
# ============================================================================

# ============================================================================
# æ—¥å¿—é…ç½® - ç»“æ„åŒ–è¾“å‡º
# ============================================================================

class StructuredFormatter(logging.Formatter):
    """ç»“æ„åŒ–æ—¥å¿—æ ¼å¼åŒ–å™¨ï¼Œä¾¿äºè¿½è¸ªç‰¹å®šå•†å“"""
    
    def format(self, record):
        # ä»æ¶ˆæ¯ä¸­æå–good_idï¼ˆå¦‚æœæœ‰[good_id=XXX]æ ¼å¼ï¼‰
        msg = record.getMessage()
        
        # åŸºç¡€æ ¼å¼
        base_time = self.formatTime(record, '%H:%M:%S')
        level = record.levelname
        
        # æ ¹æ®æ—¥å¿—çº§åˆ«ä½¿ç”¨ä¸åŒé¢œè‰²ï¼ˆå¦‚æœæ”¯æŒï¼‰
        level_colors = {
            'DEBUG': 'ğŸ”µ',
            'INFO': 'ğŸŸ¢',
            'WARNING': 'ğŸŸ¡',
            'ERROR': 'ğŸ”´'
        }
        level_icon = level_colors.get(level, '  ')
        
        # æ ¼å¼: [æ—¶é—´] ğŸŸ¢ [æ¨¡å—.å‡½æ•°] [good_id=XXX] æ¶ˆæ¯
        func_name = f"{record.name.split('.')[-1]}.{record.funcName}" if record.funcName else record.name
        
        return f"[{base_time}] {level_icon} [{func_name}] {msg}"

LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
logging.basicConfig(
    level=logging.INFO,
    format=LOG_FORMAT,
    handlers=[
        logging.FileHandler('/tmp/prediction_service.log'),
        logging.StreamHandler()
    ]
)

# è®¾ç½®è‡ªå®šä¹‰æ ¼å¼åŒ–å™¨ç”¨äºæ§åˆ¶å°è¾“å‡º
console_handler = logging.StreamHandler()
console_handler.setFormatter(StructuredFormatter())
logger = logging.getLogger(__name__)
logger.handlers.clear()
logger.addHandler(console_handler)

# åŒæ—¶ä¿ç•™æ–‡ä»¶è¾“å‡º
file_handler = logging.FileHandler('/tmp/prediction_service.log')
file_handler.setFormatter(logging.Formatter(LOG_FORMAT))
logger.addHandler(file_handler)

DB_CONFIG = {
    'host': 'localhost',
    # 'host': '	192.3.81.194',
    'user': 'root',
    'password': 'Wyj250413.',
    'database': 'csgo_trader',
    'charset': 'utf8mb4'
}

CACHE_DIR = Path('/root/csgo_prediction/.cache')
# CACHE_DIR = Path('/Users/user/Downloads/csgoAuto/.cache')
CACHE_DIR.mkdir(parents=True, exist_ok=True)
MODEL_DIR = CACHE_DIR / 'models'
MODEL_DIR.mkdir(exist_ok=True)
METRICS_DIR = CACHE_DIR / 'metrics'
METRICS_DIR.mkdir(exist_ok=True)
ALERTS_DIR = CACHE_DIR / 'alerts'
ALERTS_DIR.mkdir(exist_ok=True)

# Flask åº”ç”¨
app = Flask(__name__)
CORS(app)

# åˆå§‹åŒ–æ•°æ®è´¨é‡æ£€æŸ¥å’Œå‘Šè­¦ç³»ç»Ÿ
QUALITY_CHECKER = DataQualityChecker(outlier_method='iqr', outlier_threshold=1.5)
DRIFT_DETECTOR = DataDriftDetector(recent_ratio=0.3, drift_threshold=0.5)
DATA_CLEANER = DataCleaner()
ALERT_SYSTEM = DriftAlertSystem(alert_dir=ALERTS_DIR)
RETRAINING_TRIGGER = RetrainingTrigger(models_dir=MODEL_DIR)

# ============================================================================
# èµ„æºç®¡ç† (ç¼“å­˜, é”, è¿æ¥æ± )
# ============================================================================

class SimpleConnectionPool:
    """ç®€å•çš„æ•°æ®åº“è¿æ¥æ± """
    def __init__(self, max_size=10):
        self.pool = queue.Queue(maxsize=max_size)
        self.max_size = max_size
        self.current_size = 0
        self.lock = Lock()
        
    def get_connection(self):
        try:
            # å°è¯•ä»æ± ä¸­è·å–
            return self.pool.get_nowait()
        except queue.Empty:
            # æ± ç©ºï¼Œå°è¯•æ–°å»º
            with self.lock:
                if self.current_size < self.max_size:
                    conn = pymysql.connect(**DB_CONFIG)
                    self.current_size += 1
                    return conn
            # è¾¾åˆ°æœ€å¤§è¿æ¥æ•°ï¼Œé˜»å¡ç­‰å¾…
            return self.pool.get()

    def release_connection(self, conn):
        try:
            # æ£€æŸ¥è¿æ¥æ˜¯å¦å­˜æ´»
            conn.ping(reconnect=True)
            self.pool.put_nowait(conn)
        except Exception:
            # è¿æ¥å·²æ­»ï¼Œä¸¢å¼ƒ
            with self.lock:
                self.current_size -= 1
                try:
                    conn.close()
                except:
                    pass

class ModelCacheManager:
    """çº¿ç¨‹å®‰å…¨çš„æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨ (LRUç®€åŒ–ç‰ˆ)"""
    def __init__(self, max_size=1000):
        self.cache = {}
        self.access_time = {}
        self.max_size = max_size
        self.global_lock = Lock()
        self.item_locks = defaultdict(Lock) # ç»†ç²’åº¦é”
        
    def get_lock(self, good_id):
        with self.global_lock:
            return self.item_locks[good_id]
            
    def get(self, good_id):
        with self.global_lock:
            if good_id in self.cache:
                self.access_time[good_id] = time.time()
                return self.cache[good_id]
            return None
            
    def put(self, good_id, model):
        with self.global_lock:
            if len(self.cache) >= self.max_size and good_id not in self.cache:
                # æ¸…ç†æœ€ä¹…æœªä½¿ç”¨çš„
                oldest_id = min(self.access_time, key=self.access_time.get)
                del self.cache[oldest_id]
                del self.access_time[oldest_id]
                # æ³¨æ„ï¼šitem_locks ä¸æ¸…ç†ï¼Œä¸ºäº†å®‰å…¨
                
            self.cache[good_id] = model
            self.access_time[good_id] = time.time()

    def clear(self):
        with self.global_lock:
            self.cache.clear()
            self.access_time.clear()
            return True
            
    def size(self):
        with self.global_lock:
            return len(self.cache)

# å…¨å±€èµ„æº
DB_POOL = SimpleConnectionPool(max_size=20)
CACHE_MANAGER = ModelCacheManager(max_size=500)

# ============================================================================
# æ•°æ®åº“æ“ä½œ
# ============================================================================

def fetch_historical_data(good_id, days=30):
    """ä»æ•°æ®åº“è·å–å†å²ä»·æ ¼æ•°æ® (ä½¿ç”¨è¿æ¥æ± )"""
    conn = DB_POOL.get_connection()
    if not conn:
        return None

    try:
        cursor = conn.cursor()
        query = """
        SELECT created_at, yyyp_buy_price, yyyp_sell_price,
               yyyp_buy_count, yyyp_sell_count
        FROM csqaq_good_snapshots
        WHERE good_id = %s
        AND created_at >= DATE_SUB(NOW(), INTERVAL %s DAY)
        AND yyyp_buy_price > 0 AND yyyp_sell_price > 0
        ORDER BY created_at ASC
        """

        cursor.execute(query, (good_id, days))
        results = cursor.fetchall()
        cursor.close()

        if not results:
            return None

        df = pd.DataFrame(results, columns=[
            'timestamp', 'buy_price', 'sell_price',
            'buy_orders', 'sell_orders'
        ])

        df['timestamp'] = pd.to_datetime(df['timestamp'])
        return df.sort_values('timestamp').reset_index(drop=True)
    except Exception as e:
        logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        return None
    finally:
        DB_POOL.release_connection(conn)


# ============================================================================
# ç‰¹å¾å·¥ç¨‹
# ============================================================================

def prepare_features(df):
    """ä¸º XGBoost å‡†å¤‡ç‰¹å¾"""
    df_features = df.copy()

    # æ—¶é—´ç‰¹å¾
    df_features['day_of_week'] = df_features['timestamp'].dt.dayofweek
    df_features['day_of_month'] = df_features['timestamp'].dt.day
    df_features['days_since_start'] = (df_features['timestamp'] - df_features['timestamp'].min()).dt.days

    # ä»·æ ¼ç‰¹å¾
    df_features['price_range'] = df_features['sell_price'] - df_features['buy_price']
    df_features['total_orders'] = df_features['buy_orders'] + df_features['sell_orders']
    df_features['order_ratio'] = df_features['buy_orders'] / (df_features['sell_orders'] + 1)

    # ç§»åŠ¨å¹³å‡
    df_features['buy_price_ma3'] = df_features['buy_price'].rolling(3, min_periods=1).mean()
    df_features['sell_price_ma3'] = df_features['sell_price'].rolling(3, min_periods=1).mean()

    # ä»·æ ¼å˜åŒ–ç‡
    df_features['price_change'] = df_features['sell_price'].pct_change().fillna(0)
    df_features['price_change_ma'] = df_features['price_change'].rolling(3, min_periods=1).mean()

    # ===== æ–°å¢ï¼šé•¿æœŸè¶‹åŠ¿ç‰¹å¾ï¼ˆå…³é”®ï¼ï¼‰=====
    # 7å¤©å’Œ30å¤©çš„ä»·æ ¼è¶‹åŠ¿ï¼ˆç›¸å¯¹äºå½“å‰ä»·æ ¼ï¼‰
    df_features['trend_7d'] = df_features['sell_price'].pct_change(periods=min(7, len(df_features))).fillna(0)
    df_features['trend_30d'] = df_features['sell_price'].pct_change(periods=min(30, len(df_features))).fillna(0)
    
    # ä»·æ ¼åŠ¨é‡ï¼š7å¤©ç§»åŠ¨å¹³å‡ vs 30å¤©ç§»åŠ¨å¹³å‡
    df_features['ma7'] = df_features['sell_price'].rolling(7, min_periods=1).mean()
    df_features['ma30'] = df_features['sell_price'].rolling(30, min_periods=1).mean()
    df_features['momentum'] = (df_features['ma7'] - df_features['ma30']) / df_features['ma30']
    
    # ä»·æ ¼ç›¸å¯¹ä½ç½®ï¼ˆå½“å‰ä»·æ ¼ç›¸å¯¹äº30å¤©æœ€é«˜/æœ€ä½çš„ä½ç½®ï¼‰
    df_features['price_max_30d'] = df_features['sell_price'].rolling(30, min_periods=1).max()
    df_features['price_min_30d'] = df_features['sell_price'].rolling(30, min_periods=1).min()
    df_features['price_position'] = (df_features['sell_price'] - df_features['price_min_30d']) / (df_features['price_max_30d'] - df_features['price_min_30d'] + 0.01)

    # å¤„ç†ç¼ºå¤±å€¼
    df_features = df_features.fillna(method='ffill').fillna(method='bfill')

    return df_features


# ============================================================================
# æ¨¡å‹è®­ç»ƒä¸æŒä¹…åŒ–
# ============================================================================

class PredictionModel:
    """é¢„æµ‹æ¨¡å‹é›†åˆ - æ”¯æŒå¢é‡è®­ç»ƒå’ŒæŒä¹…åŒ–"""

    # åŸºç¡€ç‰¹å¾ï¼ˆå…¼å®¹æ—§æ¨¡å‹ï¼‰
    FEATURE_COLS_BASE = [
        'day_of_week', 'day_of_month', 'days_since_start',
        'price_range', 'total_orders', 'order_ratio',
        'buy_price_ma3', 'sell_price_ma3', 'price_change_ma'
    ]
    
    # æ–°å¢è¶‹åŠ¿ç‰¹å¾
    FEATURE_COLS_TREND = [
        'trend_7d', 'trend_30d', 'momentum', 'price_position'
    ]
    
    # æ‰€æœ‰ç‰¹å¾
    FEATURE_COLS = FEATURE_COLS_BASE + FEATURE_COLS_TREND

    def __init__(self, good_id):
        self.good_id = good_id
        self.lr = None
        self.prophet = None
        self.xgb = None
        self.last_price = None
        self.last_timestamp = None
        self.train_size = 0
        self.weights = {'lr': 0.2, 'prophet': 0.3, 'xgb': 0.5} # é»˜è®¤æƒé‡
        self.model_version = '2.1'  # å½“å‰æ¨¡å‹ç‰ˆæœ¬
        self.feature_set = 'full'  # ç‰¹å¾é›†æ ‡è¯†ï¼š'base' æˆ– 'full'

        # è´¨é‡æŒ‡æ ‡
        self.metrics = {
            'lr_mse': None, 'lr_mae': None, 'lr_mape': None,
            'xgb_mse': None, 'xgb_mae': None, 'xgb_mape': None,
            'prophet_mse': None, 'prophet_mae': None, 'prophet_mape': None,
            'ensemble_mse': None, 'ensemble_mae': None, 'ensemble_mape': None,
            'training_time': None,
            'training_count': 0,
            'last_training': None
        }

    def get_model_path(self):
        """è·å–æ¨¡å‹ä¿å­˜è·¯å¾„"""
        return MODEL_DIR / f"model_{self.good_id}.pkl"

    def get_metrics_path(self):
        """è·å–æŒ‡æ ‡ä¿å­˜è·¯å¾„"""
        return METRICS_DIR / f"metrics_{self.good_id}.json"

    def save_model(self):
        """ä¿å­˜æ¨¡å‹åˆ°ç£ç›˜"""
        try:
            model_data = {
                'lr': self.lr,
                'prophet': self.prophet,
                'xgb': self.xgb,
                'last_price': self.last_price,
                'last_timestamp': self.last_timestamp,
                'train_size': self.train_size,
                'metrics': self.metrics,
                'weights': self.weights,
                'model_version': self.model_version,
                'feature_set': self.feature_set,
                'feature_cols': self.FEATURE_COLS,  # ä¿å­˜å…·ä½“çš„ç‰¹å¾åˆ—
                'feature_cols_count': len(self.FEATURE_COLS)
            }
            with open(self.get_model_path(), 'wb') as f:
                pickle.dump(model_data, f)
            training_strategy = self.metrics.get('training_strategy', 'unknown')
            ensemble_mape = self.metrics.get('ensemble_mape', 0)
            logger.info(f"[good_id={self.good_id}] ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ | è®­ç»ƒæ¬¡æ•°={self.metrics.get('training_count', 0)} | ç­–ç•¥={training_strategy} | MAPE={ensemble_mape:.4f}")
        except Exception as e:
            logger.error(f"[good_id={self.good_id}] æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")

    def load_model(self):
        """ä»ç£ç›˜åŠ è½½æ¨¡å‹"""
        try:
            model_path = self.get_model_path()
            if not model_path.exists():
                # logger.info(f"[good_id={self.good_id}] æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ¨¡å‹")
                return False

            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)

            # æ£€æŸ¥æ¨¡å‹ç‰ˆæœ¬å’Œç‰¹å¾å…¼å®¹æ€§
            saved_version = model_data.get('model_version', '2.0')
            saved_feature_set = model_data.get('feature_set', None)  # å¯èƒ½ä¸ºNoneï¼ˆæ—§æ¨¡å‹ï¼‰
            saved_feature_cols = model_data.get('feature_cols', None)  # å¯èƒ½ä¸ºNoneï¼ˆæ—§æ¨¡å‹ï¼‰
            current_feature_cols = self.FEATURE_COLS
            
            # å¦‚æœæ²¡æœ‰ä¿å­˜ç‰¹å¾é›†ä¿¡æ¯ï¼ˆæ—§æ¨¡å‹ï¼‰ï¼Œæ ¹æ®ä¿å­˜çš„ç‰¹å¾åˆ—è¡¨æ¥åˆ¤æ–­
            if saved_feature_set is None:
                # æ—§æ¨¡å‹æ²¡æœ‰feature_setå­—æ®µï¼Œéœ€è¦é‡æ–°è®­ç»ƒ
                saved_count = len(saved_feature_cols) if saved_feature_cols else 9
                logger.warning(f"[good_id={self.good_id}] æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬æ¨¡å‹({saved_count}ç‰¹å¾)ï¼Œå‡çº§åˆ°æ–°ç‰ˆæœ¬(13ç‰¹å¾)ï¼Œå°†é‡æ–°è®­ç»ƒ")
                return False
            
            # å¦‚æœç‰¹å¾é›†æˆ–ç‰¹å¾åˆ—è¡¨ä¸åŒ¹é…ï¼Œè¯´æ˜éœ€è¦é‡æ–°è®­ç»ƒ
            if saved_feature_set != 'full' or saved_feature_cols != current_feature_cols:
                saved_count = len(saved_feature_cols) if saved_feature_cols else 0
                logger.warning(f"[good_id={self.good_id}] ç‰¹å¾é›†ä¸åŒ¹é…: ä¿å­˜={saved_count}ä¸ªç‰¹å¾, å½“å‰=13ä¸ªç‰¹å¾ï¼Œå°†é‡æ–°è®­ç»ƒ")
                return False
            
            self.lr = model_data['lr']
            self.prophet = model_data['prophet']
            self.xgb = model_data['xgb']
            self.last_price = model_data['last_price']
            self.last_timestamp = model_data['last_timestamp']
            self.train_size = model_data['train_size']
            # åˆå¹¶åŠ è½½çš„metricsï¼Œç¡®ä¿ä¿ç•™å·²æœ‰çš„è®­ç»ƒè®¡æ•°
            loaded_metrics = model_data.get('metrics', {})
            self.metrics.update(loaded_metrics)  # ç”¨updateæ›¿ä»£get+èµ‹å€¼ï¼Œä¿ç•™åˆå§‹åŒ–çš„é»˜è®¤å€¼
            self.weights = model_data.get('weights', self.weights)
            self.model_version = model_data.get('model_version', '2.0')
            self.feature_set = 'full'  # æˆåŠŸåŠ è½½çš„æ¨¡å‹ä¸€å®šæ˜¯'full'ç‰¹å¾é›†

            training_count = self.metrics.get('training_count', 0)
            last_training = self.metrics.get('last_training', 'æœªçŸ¥')
            ensemble_mape = self.metrics.get('ensemble_mape', 0)
            logger.info(f"[good_id={self.good_id}] âœ“ æ¨¡å‹å·²ä»ç£ç›˜åŠ è½½ | è®­ç»ƒæ¬¡æ•°={training_count} | æœ€åæ›´æ–°={last_training[:10]} | MAPE={ensemble_mape:.4f}")
            return True
        except Exception as e:
            logger.error(f"[good_id={self.good_id}] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def save_metrics(self):
        """ä¿å­˜æŒ‡æ ‡åˆ°JSONæ–‡ä»¶"""
        try:
            metrics_data = {
                'good_id': self.good_id,
                'timestamp': datetime.now().isoformat(),
                'metrics': self.metrics,
                'weights': self.weights
            }
            with open(self.get_metrics_path(), 'w') as f:
                json.dump(metrics_data, f, indent=2, default=str)
            # logger.info(f"[good_id={self.good_id}] æŒ‡æ ‡å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"[good_id={self.good_id}] æŒ‡æ ‡ä¿å­˜å¤±è´¥: {e}")

    def _calculate_metrics(self, y_true, y_pred, prefix=''):
        """è®¡ç®—MSEã€MAEã€MAPE"""
        try:
            mse = mean_squared_error(y_true, y_pred)
            mae = mean_absolute_error(y_true, y_pred)
            # MAPEå®¹æ˜“è¢«é›¶å€¼å½±å“ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            mape = mean_absolute_percentage_error(y_true, y_pred) if len(y_true) > 0 else 0

            return {
                f'{prefix}mse': float(mse),
                f'{prefix}mae': float(mae),
                f'{prefix}mape': float(mape)
            }
        except Exception as e:
            logger.warning(f"[good_id={self.good_id}] æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {}
    
    def _update_weights(self):
        """æ ¹æ®éªŒè¯é›†MAPEåŠ¨æ€è°ƒæ•´æƒé‡ (Inverse Variance Weightingå˜ä½“)"""
        try:
            # è·å–å„æ¨¡å‹MAPEï¼Œè‹¥æ— åˆ™ç»™ä¸€ä¸ªè¾ƒå¤§é»˜è®¤å€¼(1.0)
            mapes = {
                'lr': self.metrics.get('lr_mape') or 1.0,
                'prophet': self.metrics.get('prophet_mape') or 1.0,
                'xgb': self.metrics.get('xgb_mape') or 1.0
            }
            
            # è®¡ç®—å€’æ•° (è¯¯å·®è¶Šå°æƒé‡è¶Šå¤§)
            # åŠ ä¸€ä¸ªå°epsiloné˜²æ­¢é™¤é›¶
            inv_mapes = {k: 1.0 / (v + 0.001) for k, v in mapes.items()}
            total_inv = sum(inv_mapes.values())
            
            if total_inv > 0:
                old_weights = self.weights.copy()
                self.weights = {k: v / total_inv for k, v in inv_mapes.items()}
                logger.info(f"[good_id={self.good_id}] æƒé‡æ›´æ–°: LR {old_weights['lr']:.2f}â†’{self.weights['lr']:.2f} | Prophet {old_weights['prophet']:.2f}â†’{self.weights['prophet']:.2f} | XGB {old_weights['xgb']:.2f}â†’{self.weights['xgb']:.2f}")
            else:
                self.weights = {'lr': 0.2, 'prophet': 0.3, 'xgb': 0.5} # å›é€€é»˜è®¤

        except Exception as e:
            logger.warning(f"[good_id={self.good_id}] æƒé‡è®¡ç®—å¤±è´¥: {e}")
            self.weights = {'lr': 0.2, 'prophet': 0.3, 'xgb': 0.5}
    
    def _log_metrics_comparison(self, strategy, training_time):
        """è®°å½•è¯¦ç»†çš„è®­ç»ƒæ•ˆæœå¯¹æ¯”"""
        try:
            # æå–å…³é”®æŒ‡æ ‡
            lr_mape = self.metrics.get('lr_mape', 0)
            prophet_mape = self.metrics.get('prophet_mape', 0)
            xgb_mape = self.metrics.get('xgb_mape', 0)
            ensemble_mape = self.metrics.get('ensemble_mape', 0)
            
            lr_mae = self.metrics.get('lr_mae', 0)
            xgb_mae = self.metrics.get('xgb_mae', 0)
            ensemble_mae = self.metrics.get('ensemble_mae', 0)
            
            training_count = self.metrics.get('training_count', 0)
            
            # æ„å»ºå¯¹æ¯”ä¿¡æ¯
            logger.info(f"[good_id={self.good_id}] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            logger.info(f"[good_id={self.good_id}] ğŸ“Š è®­ç»ƒå®Œæˆ (ç­–ç•¥={strategy}, æ¬¡æ•°={training_count}, è€—æ—¶={training_time:.2f}s)")
            logger.info(f"[good_id={self.good_id}] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            logger.info(f"[good_id={self.good_id}] MAPEç²¾åº¦ (è¶Šå°è¶Šå¥½):")
            logger.info(f"[good_id={self.good_id}]   â€¢ çº¿æ€§å›å½’ LR    : {lr_mape:.4f}")
            logger.info(f"[good_id={self.good_id}]   â€¢ Propheté¢„æµ‹   : {prophet_mape:.4f}")
            logger.info(f"[good_id={self.good_id}]   â€¢ XGBoost       : {xgb_mape:.4f}")
            logger.info(f"[good_id={self.good_id}]   â€¢ é›†æˆé¢„æµ‹ ğŸ“ˆ    : {ensemble_mape:.4f} (æƒé‡: LR={self.weights['lr']:.2f}, Prophet={self.weights['prophet']:.2f}, XGB={self.weights['xgb']:.2f})")
            logger.info(f"[good_id={self.good_id}] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            logger.info(f"[good_id={self.good_id}] MAEç»å¯¹è¯¯å·® (çœŸå®ä»·æ ¼æ³¢åŠ¨):")
            logger.info(f"[good_id={self.good_id}]   â€¢ çº¿æ€§å›å½’ LR    : {lr_mae:.4f}")
            logger.info(f"[good_id={self.good_id}]   â€¢ XGBoost       : {xgb_mae:.4f}")
            logger.info(f"[good_id={self.good_id}]   â€¢ é›†æˆé¢„æµ‹ ğŸ“ˆ    : {ensemble_mae:.4f}")
            logger.info(f"[good_id={self.good_id}] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            
        except Exception as e:
            logger.warning(f"[good_id={self.good_id}] æŒ‡æ ‡è®°å½•å¤±è´¥: {e}")

    def train(self, df):
        """æ™ºèƒ½è®­ç»ƒï¼šæ ¹æ®æ•°æ®æ¼‚ç§»å’Œæ¨¡å‹å¹´é¾„é€‰æ‹©è®­ç»ƒç­–ç•¥"""
        logger.info(f"[good_id={self.good_id}] ğŸ¯ å¼€å§‹è®­ç»ƒæµç¨‹ | æ•°æ®ç‚¹={len(df)}")
        
        if len(df) < 10:
            logger.warning(f"[good_id={self.good_id}] âš ï¸  æ•°æ®ä¸è¶³: {len(df)} < 10")
            return False

        try:
            # ===== 0. åŸºç¡€æ£€æŸ¥ =====
            last_timestamp = df.iloc[-1]['timestamp']
            if last_timestamp < datetime.now() - timedelta(hours=24):
                logger.warning(f"[good_id={self.good_id}] âš ï¸  æ•°æ®è¿‡æ—§: æœ€åæ›´æ–°äº {last_timestamp} ({(datetime.now() - last_timestamp).total_seconds() / 3600:.1f}å°æ—¶å‰)")
                return False

            latest_sell_orders = df.iloc[-1]['sell_orders']
            if latest_sell_orders < 90:
                logger.warning(f"[good_id={self.good_id}] âš ï¸  å–å•è¿‡å°‘: {latest_sell_orders} < 90")
                return False

            # ===== 1. æ•°æ®è´¨é‡æ£€æŸ¥ =====
            quality_report = QUALITY_CHECKER.check_quality(df, self.good_id)
            logger.info(f"[good_id={self.good_id}] ğŸ“Š æ•°æ®è´¨é‡: {quality_report.quality_level} | ç¼ºå¤±å€¼={quality_report.missing_ratio:.2%} | å¼‚å¸¸å€¼={quality_report.outlier_count}")

            # æ£€æŸ¥12å°æ—¶å†…ä»·æ ¼æ˜¯å¦å®Œå…¨ç›¸åŒ
            recent_6h = df[df['timestamp'] >= df['timestamp'].max() - timedelta(hours=12)]
            if len(recent_6h) > 0 and recent_6h['sell_price'].nunique() == 1:
                logger.warning(f"[good_id={self.good_id}] âš ï¸  æ£€æµ‹åˆ°ä»·æ ¼åœæ»: è¿‘12å°æ—¶ä»·æ ¼æ— å˜åŒ–")
                return False

            # æ•°æ®è´¨é‡ä¸¥é‡æ—¶æ¸…ç†
            if quality_report.quality_level == 'critical':
                logger.warning(f"[good_id={self.good_id}] ğŸ§¹ æ•°æ®è´¨é‡ä¸¥é‡ï¼Œæ‰§è¡Œæ•°æ®æ¸…ç†...")
                df_clean, clean_stats = DATA_CLEANER.clean_data(df)
                if len(df_clean) >= 10:
                    logger.info(f"[good_id={self.good_id}] âœ“ æ•°æ®æ¸…ç†å®Œæˆ: {clean_stats}")
                    df = df_clean
                else:
                    logger.error(f"[good_id={self.good_id}] âŒ æ•°æ®æ¸…ç†åæ•°æ®ä¸è¶³: {len(df_clean)}")
                    return False

            # ===== 2. æ•°æ®æ¼‚ç§»æ£€æµ‹ =====
            drift_report = DRIFT_DETECTOR.detect_drift(df['sell_price'].values)
            drift_report_dict = asdict(drift_report)
            drift_report_dict['good_id'] = self.good_id
            from data_quality_monitor import DataDriftReport
            drift_report = DataDriftReport(**drift_report_dict)

            # ===== 3. ç”Ÿæˆå‘Šè­¦ =====
            alerts = ALERT_SYSTEM.check_alerts(
                self.good_id,
                quality_report=asdict(quality_report),
                drift_report=asdict(drift_report),
                performance_metrics=self.metrics
            )

            if alerts:
                logger.warning(f"[good_id={self.good_id}] âš ï¸  æ•°æ®å‘Šè­¦ ({len(alerts)} ä¸ª):")
                for alert in alerts:
                    alert_icon = 'ğŸ”´' if alert.alert_level == 'critical' else 'ğŸŸ¡'
                    logger.warning(f"[good_id={self.good_id}]   {alert_icon} [{alert.alert_level.upper()}] {alert.title}")
                ALERT_SYSTEM.save_alerts(alerts)

            # ===== 4. æ™ºèƒ½è®­ç»ƒå†³ç­– =====
            training_strategy = self._decide_training_strategy(df, drift_report, quality_report)
            
            if training_strategy == 'skip':
                logger.info(f"[good_id={self.good_id}] æ•°æ®ç¨³å®šï¼Œæ¨¡å‹è¾ƒæ–°ï¼Œè·³è¿‡è®­ç»ƒ")
                return True
            elif training_strategy == 'incremental':
                logger.info(f"[good_id={self.good_id}] æ‰§è¡Œå¢é‡è®­ç»ƒ (ä¸­åº¦æ¼‚ç§»)")
                return self._incremental_train(df, drift_report, quality_report)
            else:  # 'full'
                logger.info(f"[good_id={self.good_id}] æ‰§è¡Œå…¨é‡è®­ç»ƒ (ä¸¥é‡æ¼‚ç§»æˆ–é¦–æ¬¡è®­ç»ƒ)")
                return self._full_retrain(df, drift_report, quality_report)

        except Exception as e:
            logger.error(f"[good_id={self.good_id}] è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _decide_training_strategy(self, df, drift_report, quality_report):
        """å†³å®šè®­ç»ƒç­–ç•¥: 'skip', 'incremental', 'full'"""
        # 1. å¦‚æœæ²¡æœ‰ç°æœ‰æ¨¡å‹ï¼Œå¿…é¡»å…¨é‡è®­ç»ƒ
        if self.xgb is None or self.lr is None or self.prophet is None:
            logger.info(f"[good_id={self.good_id}] ğŸ“Œ é¦–æ¬¡è®­ç»ƒï¼šæ— ç°æœ‰æ¨¡å‹")
            return 'full'
        
        # 2. æ£€æŸ¥æ¨¡å‹å¹´é¾„
        model_age_days = 999
        if self.last_timestamp:
            model_age = datetime.now() - self.last_timestamp
            model_age_days = model_age.total_seconds() / 86400
        
        # 3. è·å–æ¼‚ç§»ç¨‹åº¦
        drift_level = drift_report.drift_level  # 'none', 'mild', 'moderate', 'severe'
        ks_statistic = drift_report.ks_statistic
        
        logger.info(f"[good_id={self.good_id}] ğŸ” è®­ç»ƒå†³ç­–è¯„ä¼° | æ¨¡å‹å¹´é¾„={model_age_days:.1f}å¤© | æ¼‚ç§»åº¦={drift_level}(KS={ks_statistic:.3f})")
        
        # 4. å†³ç­–é€»è¾‘
        if model_age_days > 7:
            # æ¨¡å‹å¤ªæ—§ (>7å¤©)ï¼Œå…¨é‡é‡è®­ç»ƒ
            logger.info(f"[good_id={self.good_id}] ğŸ“Œ å†³ç­–: å…¨é‡è®­ç»ƒ (åŸå› : æ¨¡å‹å¤ªæ—§ {model_age_days:.1f}å¤© > 7å¤©)")
            return 'full'
        
        if drift_level == 'severe' or ks_statistic > 0.5:
            # ä¸¥é‡æ¼‚ç§»ï¼Œå…¨é‡é‡è®­ç»ƒ
            logger.info(f"[good_id={self.good_id}] ğŸ“Œ å†³ç­–: å…¨é‡è®­ç»ƒ (åŸå› : ä¸¥é‡æ¼‚ç§» KS={ks_statistic:.3f} > 0.5)")
            return 'full'
        
        if drift_level == 'moderate' or (0.3 < ks_statistic <= 0.5):
            # ä¸­åº¦æ¼‚ç§»ï¼Œå¢é‡è®­ç»ƒ
            logger.info(f"[good_id={self.good_id}] ğŸ“Œ å†³ç­–: å¢é‡è®­ç»ƒ (åŸå› : ä¸­åº¦æ¼‚ç§» KS={ks_statistic:.3f})")
            return 'incremental'
        
        if model_age_days > 3:
            # æ¨¡å‹ç¨æ—§ (3-7å¤©)ï¼Œå¢é‡è®­ç»ƒ
            logger.info(f"[good_id={self.good_id}] ğŸ“Œ å†³ç­–: å¢é‡è®­ç»ƒ (åŸå› : æ¨¡å‹åæ—§ {model_age_days:.1f}å¤©)")
            return 'incremental'
        
        # è½»å¾®æˆ–æ— æ¼‚ç§»ï¼Œæ¨¡å‹è¾ƒæ–°ï¼Œè·³è¿‡è®­ç»ƒ
        logger.info(f"[good_id={self.good_id}] ğŸ“Œ å†³ç­–: è·³è¿‡è®­ç»ƒ (åŸå› : æ•°æ®ç¨³å®š KS={ks_statistic:.3f}, æ¨¡å‹æ–°é²œ {model_age_days:.1f}å¤©)")
        return 'skip'

    def _full_retrain(self, df, drift_report, quality_report):
        """å…¨é‡é‡è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        train_start = time.time()
        
        try:
            # æ”¹è¿›: ä½¿ç”¨80%è®­ç»ƒé›†ï¼Œè®©æ¨¡å‹çœ‹åˆ°æ›´å¤šè¿‘æœŸæ•°æ®
            split_point = int(len(df) * 0.8)
            df_train = df[:split_point].copy()
            df_test = df[split_point:].copy()
            self.train_size = len(df_train)

            self.last_price = df.iloc[-1]['sell_price']
            self.last_timestamp = df.iloc[-1]['timestamp']

            y_test = df_test['sell_price'].values

            # ===== çº¿æ€§å›å½’ (å…¨æ–°æ¨¡å‹ - åŠ æƒè®­ç»ƒï¼Œè¿‘æœŸæ•°æ®æƒé‡æ›´é«˜) =====
            y_train = df_train['sell_price'].values
            X_train = np.arange(len(y_train)).reshape(-1, 1)
            
            # æŒ‡æ•°è¡°å‡æƒé‡: è¿‘æœŸæ•°æ®æƒé‡é«˜ï¼Œè¿œæœŸæ•°æ®æƒé‡ä½
            # æƒé‡èŒƒå›´: [0.1, 1.0]ï¼Œæœ€è¿‘çš„æ•°æ®æƒé‡ä¸º1.0
            weights = np.exp(np.linspace(-2, 0, len(y_train)))
            
            self.lr = LinearRegression()  # åˆ›å»ºæ–°æ¨¡å‹
            self.lr.fit(X_train, y_train, sample_weight=weights)

            X_test = np.arange(len(y_train), len(y_train) + len(y_test)).reshape(-1, 1)
            y_pred_lr = self.lr.predict(X_test)
            lr_metrics = self._calculate_metrics(y_test, y_pred_lr, 'lr_')
            self.metrics.update(lr_metrics)

            # ===== Prophet (å…¨æ–°æ¨¡å‹) =====
            df_prophet = df_train[['timestamp', 'sell_price']].copy()
            df_prophet.columns = ['ds', 'y']
            self.prophet = Prophet(  # åˆ›å»ºæ–°æ¨¡å‹
                yearly_seasonality=False,
                weekly_seasonality=True,
                daily_seasonality=False,
                interval_width=0.95
            )
            self.prophet.fit(df_prophet)

            future_test = df_test[['timestamp']].copy()
            future_test.columns = ['ds']
            forecast_test = self.prophet.predict(future_test)
            y_pred_prophet = forecast_test['yhat'].values
            prophet_metrics = self._calculate_metrics(y_test, y_pred_prophet, 'prophet_')
            self.metrics.update(prophet_metrics)

            # ===== XGBoost (å…¨æ–°æ¨¡å‹) =====
            df_features = prepare_features(df_train)
            # åªé€‰æ‹©FEATURE_COLSä¸­å®é™…å­˜åœ¨çš„åˆ—
            available_cols = [col for col in self.FEATURE_COLS if col in df_features.columns]
            X_train_xgb = df_features[available_cols].values
            y_train_xgb = df_features['sell_price'].values

            self.xgb = XGBRegressor(  # åˆ›å»ºæ–°æ¨¡å‹
                n_estimators=50,
                max_depth=4,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                verbosity=0
            )
            self.xgb.fit(X_train_xgb, y_train_xgb)

            df_test_features = prepare_features(df_test)
            # åªé€‰æ‹©FEATURE_COLSä¸­å®é™…å­˜åœ¨çš„åˆ—
            available_cols = [col for col in self.FEATURE_COLS if col in df_test_features.columns]
            X_test_xgb = df_test_features[available_cols].values
            y_pred_xgb = self.xgb.predict(X_test_xgb)
            xgb_metrics = self._calculate_metrics(y_test, y_pred_xgb, 'xgb_')
            self.metrics.update(xgb_metrics)

            # ===== åŠ¨æ€æƒé‡ä¸é›†æˆé¢„æµ‹ =====
            self._update_weights()
            
            ensemble_pred = (
                y_pred_lr * self.weights['lr'] +
                y_pred_prophet * self.weights['prophet'] +
                y_pred_xgb * self.weights['xgb']
            )
            ensemble_metrics = self._calculate_metrics(y_test, ensemble_pred, 'ensemble_')
            self.metrics.update(ensemble_metrics)

            training_time = time.time() - train_start
            self.metrics['training_time'] = training_time
            # ç¡®ä¿training_countæ­£ç¡®ç´¯åŠ 
            current_count = self.metrics.get('training_count', 0)
            self.metrics['training_count'] = current_count + 1
            self.metrics['last_training'] = datetime.now().isoformat()
            self.metrics['training_strategy'] = 'full_retrain'

            self.metrics['quality_report'] = asdict(quality_report)
            self.metrics['drift_report'] = asdict(drift_report)

            # æ ‡è®°ä¸ºå®Œæ•´ç‰¹å¾é›†
            self.feature_set = 'full'
            self.save_model()
            self.save_metrics()

            self._log_metrics_comparison(strategy='full_retrain', training_time=training_time)
            return True

        except Exception as e:
            logger.error(f"[good_id={self.good_id}] å…¨é‡è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(f"[good_id={self.good_id}] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False

    def _incremental_train(self, df, drift_report, quality_report):
        """å¢é‡è®­ç»ƒï¼šä»…æ›´æ–°XGBoostï¼ŒLRå’ŒPropheté‡è®­ç»ƒ"""
        train_start = time.time()
        
        try:
            # æ”¹è¿›: ä½¿ç”¨80%è®­ç»ƒé›†ï¼Œè®©æ¨¡å‹çœ‹åˆ°æ›´å¤šè¿‘æœŸæ•°æ®
            split_point = int(len(df) * 0.8)
            df_train = df[:split_point].copy()
            df_test = df[split_point:].copy()
            self.train_size = len(df_train)

            self.last_price = df.iloc[-1]['sell_price']
            self.last_timestamp = df.iloc[-1]['timestamp']

            y_test = df_test['sell_price'].values

            # ===== çº¿æ€§å›å½’ (é‡è®­ç»ƒï¼Œè½»é‡ - åŠ æƒè®­ç»ƒ) =====
            y_train = df_train['sell_price'].values
            X_train = np.arange(len(y_train)).reshape(-1, 1)
            
            # æŒ‡æ•°è¡°å‡æƒé‡: è¿‘æœŸæ•°æ®æƒé‡é«˜
            weights = np.exp(np.linspace(-2, 0, len(y_train)))
            
            self.lr = LinearRegression()
            self.lr.fit(X_train, y_train, sample_weight=weights)

            X_test = np.arange(len(y_train), len(y_train) + len(y_test)).reshape(-1, 1)
            y_pred_lr = self.lr.predict(X_test)
            lr_metrics = self._calculate_metrics(y_test, y_pred_lr, 'lr_')
            self.metrics.update(lr_metrics)

            # ===== Prophet (é‡è®­ç»ƒï¼Œè¾ƒé‡) =====
            df_prophet = df_train[['timestamp', 'sell_price']].copy()
            df_prophet.columns = ['ds', 'y']
            self.prophet = Prophet(
                yearly_seasonality=False,
                weekly_seasonality=True,
                daily_seasonality=False,
                interval_width=0.95
            )
            self.prophet.fit(df_prophet)

            future_test = df_test[['timestamp']].copy()
            future_test.columns = ['ds']
            forecast_test = self.prophet.predict(future_test)
            y_pred_prophet = forecast_test['yhat'].values
            prophet_metrics = self._calculate_metrics(y_test, y_pred_prophet, 'prophet_')
            self.metrics.update(prophet_metrics)

            # ===== XGBoost (å¢é‡è®­ç»ƒ - å…³é”®æ”¹è¿›) =====
            df_features = prepare_features(df_train)
            # åªé€‰æ‹©FEATURE_COLSä¸­å®é™…å­˜åœ¨çš„åˆ—
            available_cols = [col for col in self.FEATURE_COLS if col in df_features.columns]
            X_train_xgb = df_features[available_cols].values
            y_train_xgb = df_features['sell_price'].values

            if self.xgb is not None:
                # åŸºäºæ—§æ¨¡å‹ç»§ç»­è®­ç»ƒ
                logger.info(f"[good_id={self.good_id}] XGBoost å¢é‡æ›´æ–°...")
                self.xgb.fit(
                    X_train_xgb, y_train_xgb,
                    xgb_model=self.xgb.get_booster()  # å¢é‡è®­ç»ƒ
                )
            else:
                # æ²¡æœ‰æ—§æ¨¡å‹ï¼Œåˆ›å»ºæ–°çš„
                self.xgb = XGBRegressor(
                    n_estimators=50,
                    max_depth=4,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    random_state=42,
                    verbosity=0
                )
                self.xgb.fit(X_train_xgb, y_train_xgb)

            df_test_features = prepare_features(df_test)
            # åªé€‰æ‹©FEATURE_COLSä¸­å®é™…å­˜åœ¨çš„åˆ—
            available_cols = [col for col in self.FEATURE_COLS if col in df_test_features.columns]
            X_test_xgb = df_test_features[available_cols].values
            y_pred_xgb = self.xgb.predict(X_test_xgb)
            xgb_metrics = self._calculate_metrics(y_test, y_pred_xgb, 'xgb_')
            self.metrics.update(xgb_metrics)

            # ===== åŠ¨æ€æƒé‡ä¸é›†æˆé¢„æµ‹ =====
            self._update_weights()
            
            ensemble_pred = (
                y_pred_lr * self.weights['lr'] +
                y_pred_prophet * self.weights['prophet'] +
                y_pred_xgb * self.weights['xgb']
            )
            ensemble_metrics = self._calculate_metrics(y_test, ensemble_pred, 'ensemble_')
            self.metrics.update(ensemble_metrics)

            training_time = time.time() - train_start
            self.metrics['training_time'] = training_time
            # ç¡®ä¿training_countæ­£ç¡®ç´¯åŠ 
            current_count = self.metrics.get('training_count', 0)
            self.metrics['training_count'] = current_count + 1
            self.metrics['last_training'] = datetime.now().isoformat()
            self.metrics['training_strategy'] = 'incremental'

            self.metrics['quality_report'] = asdict(quality_report)
            self.metrics['drift_report'] = asdict(drift_report)

            # æ ‡è®°ä¸ºå®Œæ•´ç‰¹å¾é›†
            self.feature_set = 'full'
            self.save_model()
            self.save_metrics()

            self._log_metrics_comparison(strategy='incremental', training_time=training_time)
            return True

        except Exception as e:
            logger.error(f"[good_id={self.good_id}] å¢é‡è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(f"[good_id={self.good_id}] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False

    def predict(self, days=7):
        """é¢„æµ‹æœªæ¥Nå¤©çš„ä»·æ ¼ï¼ˆä»¥å¤©ä¸ºå•ä½ï¼‰"""
        if self.lr is None or self.prophet is None or self.xgb is None:
            return None

        predictions = {
            'current_price': float(self.last_price),
            'last_timestamp': self.last_timestamp.isoformat(),
            'forecast_days': days,
            'predictions': {}
        }

        try:
            future_dates = pd.date_range(
                start=self.last_timestamp + timedelta(days=1),
                periods=days,
                freq='D'
            )

            # ===== çº¿æ€§å›å½’é¢„æµ‹ =====
            day_indices = np.arange(self.train_size, self.train_size + days).reshape(-1, 1)
            lr_pred = np.maximum(self.lr.predict(day_indices), 0)
            predictions['predictions']['lr'] = {
                'forecast': [float(p) for p in lr_pred],
                'dates': [d.isoformat() for d in future_dates],
                'model': 'LinearRegression'
            }

            # ===== Prophet é¢„æµ‹ =====
            future_df = pd.DataFrame({'ds': future_dates})
            forecast = self.prophet.predict(future_df)
            prophet_pred = np.maximum(forecast['yhat'].values, 0)
            prophet_lower = np.maximum(forecast['yhat_lower'].values, 0)
            prophet_upper = forecast['yhat_upper'].values

            predictions['predictions']['prophet'] = {
                'forecast': [float(p) for p in prophet_pred],
                'lower': [float(l) for l in prophet_lower],
                'upper': [float(u) for u in prophet_upper],
                'dates': [d.isoformat() for d in future_dates],
                'model': 'Facebook Prophet'
            }

            # ===== XGBoost é¢„æµ‹ =====
            last_features = self._generate_future_features(days)
            # åªé€‰æ‹©FEATURE_COLSä¸­å®é™…å­˜åœ¨çš„åˆ—
            available_cols = [col for col in self.FEATURE_COLS if col in last_features.columns]
            X_future_xgb = last_features[available_cols].values
            xgb_pred = np.maximum(self.xgb.predict(X_future_xgb), 0)

            predictions['predictions']['xgb'] = {
                'forecast': [float(p) for p in xgb_pred],
                'dates': [d.isoformat() for d in future_dates],
                'model': 'XGBoost'
            }

            # ===== é›†æˆé¢„æµ‹ (åŠ¨æ€æƒé‡) =====
            ensemble_pred = (
                np.array(predictions['predictions']['lr']['forecast']) * self.weights['lr'] +
                np.array(predictions['predictions']['prophet']['forecast']) * self.weights['prophet'] +
                np.array(predictions['predictions']['xgb']['forecast']) * self.weights['xgb']
            )

            predictions['ensemble'] = {
                'forecast': [float(p) for p in ensemble_pred],
                'dates': [d.isoformat() for d in future_dates],
                'weights': self.weights,
                'model': 'Weighted Ensemble'
            }

            predictions['quality_metrics'] = {
                'ensemble_mape': self.metrics.get('ensemble_mape'),
                'ensemble_mae': self.metrics.get('ensemble_mae'),
                'training_count': self.metrics.get('training_count', 0),
                'last_training': self.metrics.get('last_training')
            }

            # ===== ç”Ÿæˆæ¨è (äº¤æ˜“ç­–ç•¥ï¼šç°åœ¨ä¹°å…¥ -> 7å¤©åå–å‡º) =====
            avg_future_price = np.mean(ensemble_pred)
            future_change_pct = ((avg_future_price - self.last_price) / self.last_price) * 100

            # è®¡ç®—è¿‘æœŸè¶‹åŠ¿ï¼ˆè¿‡å»7å¤©çš„ä»·æ ¼å˜åŒ–ï¼‰
            try:
                recent_data = fetch_historical_data(self.good_id, days=7)
                if recent_data is not None and len(recent_data) >= 2:
                    recent_start_price = recent_data.iloc[0]['sell_price']
                    recent_trend_pct = ((self.last_price - recent_start_price) / recent_start_price) * 100
                else:
                    recent_trend_pct = 0
            except:
                recent_trend_pct = 0

            # æ‰‹ç»­è´¹é˜ˆå€¼ (åŒè¾¹æ‰‹ç»­è´¹çº¦1%)
            FEE_THRESHOLD = 1.0
            PROFIT_THRESHOLD = 3.0  # æœŸæœ›æ”¶ç›Šé˜ˆå€¼
            CHASE_HIGH_THRESHOLD = 5.0  # è¿½é«˜é£é™©é˜ˆå€¼

            # æ™ºèƒ½æ¨èé€»è¾‘
            if future_change_pct > PROFIT_THRESHOLD:
                # é¢„æµ‹æœªæ¥ä¼šæ¶¨ > 3%
                if recent_trend_pct > CHASE_HIGH_THRESHOLD:
                    # è¿‘æœŸå·²ç»å¤§æ¶¨ > 5%ï¼Œè¿½é«˜é£é™©
                    recommendation = 'hold'
                    reason = f'è™½ç„¶é¢„æµ‹7å¤©åä¸Šæ¶¨{future_change_pct:.1f}%ï¼Œä½†è¿‘7å¤©å·²æ¶¨{recent_trend_pct:.1f}%ï¼Œè¿½é«˜é£é™©å¤§ï¼Œå»ºè®®è§‚æœ›'
                else:
                    # è¿‘æœŸæœªå¤§æ¶¨ï¼Œå¯ä»¥ä¹°å…¥
                    recommendation = 'buy'
                    expected_profit = future_change_pct - FEE_THRESHOLD
                    reason = f'é¢„æµ‹7å¤©åä¸Šæ¶¨{future_change_pct:.1f}%ï¼Œæ‰£é™¤æ‰‹ç»­è´¹çº¦{FEE_THRESHOLD}%ï¼Œé¢„æœŸæ”¶ç›Š{expected_profit:.1f}%ï¼Œå»ºè®®ä¹°å…¥'
            elif future_change_pct < -FEE_THRESHOLD:
                # é¢„æµ‹æœªæ¥ä¼šè·Œ
                recommendation = 'hold'
                reason = f'é¢„æµ‹7å¤©åä¸‹è·Œ{future_change_pct:.1f}%ï¼Œç°åœ¨ä¹°å…¥ä¼šäºæŸï¼Œä¸å»ºè®®æ“ä½œ'
            else:
                # é¢„æµ‹å˜åŒ–ä¸å¤§
                recommendation = 'hold'
                reason = f'é¢„æµ‹7å¤©åä»·æ ¼å˜åŒ–{future_change_pct:.1f}%ï¼Œæ”¶ç›Šä¸è¶³ä»¥è¦†ç›–æ‰‹ç»­è´¹{FEE_THRESHOLD}%ï¼Œå»ºè®®è§‚æœ›'

            # è®¡ç®—é¢„æœŸå‡€æ”¶ç›Šï¼ˆè€ƒè™‘æ‰‹ç»­è´¹ï¼‰
            # å¦‚æœé¢„æµ‹æ¶¨3%ï¼Œæ‰£é™¤æ‰‹ç»­è´¹åå‡€æ”¶ç›Šçº¦1%
            # å¦‚æœé¢„æµ‹è·Œ3%ï¼ŒåŠ ä¸Šæ‰‹ç»­è´¹åäºæŸçº¦5%
            if future_change_pct > 0:
                expected_profit = future_change_pct - FEE_THRESHOLD  # ä¸Šæ¶¨æ‰£æ‰‹ç»­è´¹
            else:
                expected_profit = future_change_pct - FEE_THRESHOLD  # ä¸‹è·Œè¿˜è¦æ‰£æ‰‹ç»­è´¹ï¼Œäºæ›´å¤š
            
            predictions['recommendation'] = {
                'action': recommendation,
                'avg_future_price': float(avg_future_price),
                'future_change_pct': float(future_change_pct),
                'recent_trend_pct': float(recent_trend_pct),
                'expected_profit': float(expected_profit),
                'reason': reason,
                'confidence': 0.95
            }

            return predictions

        except Exception as e:
            logger.error(f"[good_id={self.good_id}] é¢„æµ‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def _generate_future_features(self, days):
        """ç”Ÿæˆæœªæ¥ç‰¹å¾ï¼ˆä»¥å¤©ä¸ºå•ä½ï¼‰"""
        future_dates = pd.date_range(
            start=self.last_timestamp + timedelta(days=1),
            periods=days,
            freq='D'
        )

        # è·å–å†å²æ•°æ®æ¥è®¡ç®—è¶‹åŠ¿ç‰¹å¾
        df_hist = fetch_historical_data(self.good_id, days=30)
        
        # è®¡ç®—å½“å‰çš„è¶‹åŠ¿å€¼
        if df_hist is not None and len(df_hist) >= 30:
            recent_7d = df_hist.tail(7)['sell_price'].values
            recent_30d = df_hist.tail(30)['sell_price'].values
            trend_7d = (recent_7d[-1] - recent_7d[0]) / recent_7d[0] if len(recent_7d) > 0 else 0
            trend_30d = (recent_30d[-1] - recent_30d[0]) / recent_30d[0] if len(recent_30d) > 0 else 0
            ma7 = recent_7d.mean()
            ma30 = recent_30d.mean()
            momentum = (ma7 - ma30) / ma30 if ma30 > 0 else 0
            price_max = recent_30d.max()
            price_min = recent_30d.min()
            price_position = (self.last_price - price_min) / (price_max - price_min + 0.01)
        else:
            trend_7d = trend_30d = momentum = price_position = 0

        future_df = pd.DataFrame({
            'timestamp': future_dates,
            'day_of_week': future_dates.dayofweek,
            'day_of_month': future_dates.day,
            'days_since_start': [(d - self.last_timestamp).days for d in future_dates],
            'price_range': 0.5,
            'total_orders': 100,
            'order_ratio': 0.5,
            'buy_price_ma3': self.last_price,
            'sell_price_ma3': self.last_price,
            'price_change_ma': 0.0,
            # æ–°å¢è¶‹åŠ¿ç‰¹å¾ï¼ˆä½¿ç”¨å†å²æ•°æ®è®¡ç®—ï¼‰
            'trend_7d': trend_7d,
            'trend_30d': trend_30d,
            'momentum': momentum,
            'price_position': price_position
        })
        
        # ç¡®ä¿åªè¿”å›FEATURE_COLSä¸­éœ€è¦çš„ç‰¹å¾åˆ—
        future_df = future_df[[col for col in self.FEATURE_COLS if col in future_df.columns]]

        return future_df


# ============================================================================
# API ç«¯ç‚¹
# ============================================================================

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'ok',
        'timestamp': datetime.now().isoformat(),
        'cached_models': CACHE_MANAGER.size(),
        'version': '2.1.0-optimized',
        'features': [
            'concurrency-optimized',
            'connection-pooling',
            'dynamic-weights',
            'fee-aware-recommendation',
            'model-persistence'
        ]
    }), 200


@app.route('/api/predict/<int:good_id>', methods=['GET'])
def predict_endpoint(good_id):
    """é¢„æµ‹å•ä¸ªå•†å“"""
    try:
        days = request.args.get('days', default=7, type=int)
        if days < 1 or days > 30:
            return jsonify({'error': 'é¢„æµ‹å¤©æ•°å¿…é¡»åœ¨ 1-30 ä¹‹é—´'}), 400

        logger.info(f"[good_id={good_id}] ğŸ“¤ æ”¶åˆ°é¢„æµ‹è¯·æ±‚ | é¢„æµ‹å¤©æ•°={days}å¤©")
        
        # ä½¿ç”¨ç»†ç²’åº¦é”
        item_lock = CACHE_MANAGER.get_lock(good_id)
        
        with item_lock:
            # å°è¯•ä»ç¼“å­˜è·å–
            model = CACHE_MANAGER.get(good_id)
            
            if model is None:
                logger.info(f"[good_id={good_id}] ğŸ’¾ ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•ä»ç£ç›˜åŠ è½½æˆ–è®­ç»ƒ")
                # ç¼“å­˜æœªå‘½ä¸­ï¼Œåˆ›å»ºæ–°æ¨¡å‹
                model = PredictionModel(good_id)
                # å°è¯•åŠ è½½ç£ç›˜æ¨¡å‹
                if not model.load_model():
                    # ç£ç›˜æ— æ¨¡å‹ï¼Œéœ€è¦è®­ç»ƒ
                    logger.info(f"[good_id={good_id}] ğŸ”„ å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹...")
                    df = fetch_historical_data(good_id, days=30)
                    if df is None or len(df) < 10:
                        logger.warning(f"[good_id={good_id}] âŒ æ•°æ®ä¸è¶³: {len(df) if df is not None else 0} < 10")
                        return jsonify({'error': 'æ•°æ®ä¸è¶³'}), 400

                    if not model.train(df):
                        logger.error(f"[good_id={good_id}] âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
                        return jsonify({'error': 'æ¨¡å‹è®­ç»ƒå¤±è´¥'}), 400
                else:
                    logger.info(f"[good_id={good_id}] âœ“ ä»ç£ç›˜åŠ è½½æˆåŠŸ")
                
                # å­˜å…¥ç¼“å­˜
                CACHE_MANAGER.put(good_id, model)
            else:
                logger.debug(f"[good_id={good_id}] âš¡ æ¨¡å‹æ¥è‡ªå†…å­˜ç¼“å­˜")

            result = model.predict(days=days)
            if result is None:
                logger.error(f"[good_id={good_id}] âŒ é¢„æµ‹è®¡ç®—å¤±è´¥")
                return jsonify({'error': 'é¢„æµ‹å¤±è´¥'}), 400

            result['good_id'] = good_id
            recommendation = result.get('recommendation', {})
            action = recommendation.get('action', 'unknown')
            confidence = recommendation.get('confidence', 0)
            logger.info(f"[good_id={good_id}] âœ… é¢„æµ‹å®Œæˆ | æ¨è={action} | ç½®ä¿¡åº¦={confidence} | é¢„æœŸæ”¶ç›Š={recommendation.get('expected_profit', 0):.2f}%")
            
            return jsonify(result), 200

    except Exception as e:
        logger.error(f"[good_id={good_id}] âŒ å¼‚å¸¸: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500

def process_single_good(good_id, days):
    """å¤„ç†å•ä¸ªå•†å“çš„å‡½æ•° (ç”¨äºçº¿ç¨‹æ± )"""
    try:
        item_lock = CACHE_MANAGER.get_lock(good_id)
        with item_lock:
            model = CACHE_MANAGER.get(good_id)
            status = "cached"
            
            if model is None:
                logger.info(f"[good_id={good_id}] [æ‰¹å¤„ç†] ç¼“å­˜æœªå‘½ä¸­ï¼Œåˆ›å»ºæ–°æ¨¡å‹")
                model = PredictionModel(good_id)
                if not model.load_model():
                    logger.info(f"[good_id={good_id}] [æ‰¹å¤„ç†] ç£ç›˜æ— æ¨¡å‹ï¼Œå¼€å§‹è®­ç»ƒ...")
                    df = fetch_historical_data(good_id, days=30)
                    if df is None or len(df) < 10:
                        logger.warning(f"[good_id={good_id}] [æ‰¹å¤„ç†] âš ï¸  è·³è¿‡: æ•°æ®ä¸è¶³ ({len(df) if df is not None else 0}æ¡)")
                        return None, "skipped_no_data"

                    if not model.train(df):
                        logger.error(f"[good_id={good_id}] [æ‰¹å¤„ç†] âŒ è®­ç»ƒå¤±è´¥")
                        return None, "skipped_train_failed"
                    status = "trained"
                    logger.info(f"[good_id={good_id}] [æ‰¹å¤„ç†] âœ“ è®­ç»ƒæˆåŠŸ | MAPE={model.metrics.get('ensemble_mape', 0):.4f}")
                else:
                    status = "loaded_disk"
                    logger.info(f"[good_id={good_id}] [æ‰¹å¤„ç†] âœ“ ä»ç£ç›˜åŠ è½½ | è®­ç»ƒæ¬¡æ•°={model.metrics.get('training_count', 0)}")
                
                CACHE_MANAGER.put(good_id, model)
            else:
                logger.debug(f"[good_id={good_id}] [æ‰¹å¤„ç†] âš¡ æ¥è‡ªå†…å­˜ç¼“å­˜")

            result = model.predict(days=days)
            if result:
                result['good_id'] = good_id
                recommendation = result.get('recommendation', {})
                action = recommendation.get('action', 'unknown')
                change_pct = recommendation.get('future_change_pct', 0)
                expected_profit = recommendation.get('expected_profit', 0)
                logger.info(f"[good_id={good_id}] [æ‰¹å¤„ç†] âœ… é¢„æµ‹å®Œæˆ | æ¨è={action} | é¢„è®¡å˜åŒ–={change_pct:.2f}% | é¢„æœŸæ”¶ç›Š={expected_profit:.2f}%")
                return result, status
            else:
                logger.error(f"[good_id={good_id}] [æ‰¹å¤„ç†] âŒ é¢„æµ‹å¤±è´¥")
                return None, "predict_failed"
            
    except Exception as e:
        logger.error(f"[good_id={good_id}] [æ‰¹å¤„ç†] âŒ å¼‚å¸¸: {e}", exc_info=True)
        return None, "error"

@app.route('/api/batch-predict', methods=['POST'])
def batch_predict_endpoint():
    """æ‰¹é‡é¢„æµ‹ (å¹¶å‘ä¼˜åŒ–ç‰ˆ)"""
    try:
        data = request.get_json()
        good_ids = data.get('good_ids', [])
        days = data.get('days', 7)

        if not good_ids or len(good_ids) > 100:
            return jsonify({'error': 'å•†å“æ•°å¿…é¡»åœ¨ 1-100 ä¹‹é—´'}), 400

        batch_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        logger.info(f"")
        logger.info(f"ğŸš€ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info(f"ğŸš€ æ‰¹é‡é¢„æµ‹å¼€å§‹ [batch_id={batch_id}] | å•†å“æ•°={len(good_ids)} | é¢„æµ‹å¤©æ•°={days}å¤©")
        logger.info(f"ğŸš€ å•†å“åˆ—è¡¨: {good_ids}")
        logger.info(f"ğŸš€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
        results = []
        stats = defaultdict(int)
        processed = 0
        start_time = time.time()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        # max_workers æ ¹æ®æœºå™¨æ€§èƒ½è°ƒæ•´ï¼Œä¸€èˆ¬ CPUæ ¸å¿ƒæ•° * 2
        with ThreadPoolExecutor(max_workers=8) as executor:
            future_to_good = {executor.submit(process_single_good, gid, days): gid for gid in good_ids}
            total = len(good_ids)
            
            for future in as_completed(future_to_good):
                good_id = future_to_good[future]
                processed += 1
                try:
                    result, status = future.result()
                    stats[status] += 1
                    if result:
                        results.append(result)
                    progress_pct = (processed / total) * 100
                    elapsed_time = time.time() - start_time
                    eta_seconds = (elapsed_time / processed * (total - processed)) if processed > 0 else 0
                    logger.info(f"ğŸš€ [è¿›åº¦] {processed:2d}/{total} ({progress_pct:5.1f}%) | ETA={int(eta_seconds)}s | [{status:15s}] good_id={good_id}")
                except Exception as e:
                    logger.error(f"ğŸš€ [good_id={good_id}] âŒ çº¿ç¨‹å¼‚å¸¸: {e}", exc_info=True)
                    stats['thread_error'] += 1

        total_time = time.time() - start_time
        logger.info(f"ğŸš€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        logger.info(f"ğŸš€ âœ… æ‰¹é‡å¤„ç†å®Œæˆ | æˆåŠŸ={len(results)}/{len(good_ids)} | è€—æ—¶={total_time:.2f}s")
        logger.info(f"ğŸš€ ç»Ÿè®¡æ˜ç»†:")
        logger.info(f"ğŸš€   â€¢ trained        (æ–°è®­ç»ƒ): {stats.get('trained', 0):3d}ä¸ª")
        logger.info(f"ğŸš€   â€¢ loaded_disk    (ç£ç›˜åŠ è½½): {stats.get('loaded_disk', 0):3d}ä¸ª")
        logger.info(f"ğŸš€   â€¢ cached         (å†…å­˜ç¼“å­˜): {stats.get('cached', 0):3d}ä¸ª")
        logger.info(f"ğŸš€   â€¢ predict_failed (é¢„æµ‹å¤±è´¥): {stats.get('predict_failed', 0):3d}ä¸ª")
        logger.info(f"ğŸš€   â€¢ skipped_no_data (æ•°æ®ä¸è¶³): {stats.get('skipped_no_data', 0):3d}ä¸ª")
        logger.info(f"ğŸš€   â€¢ skipped_train_failed (è®­ç»ƒå¤±è´¥): {stats.get('skipped_train_failed', 0):3d}ä¸ª")
        logger.info(f"ğŸš€   â€¢ thread_error   (çº¿ç¨‹å¼‚å¸¸): {stats.get('thread_error', 0):3d}ä¸ª")
        logger.info(f"ğŸš€ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info(f"")

        return jsonify({
            'total_requested': len(good_ids),
            'total_success': len(results),
            'stats': stats,
            'results': results
        }), 200

    except Exception as e:
        logger.error(f"æ‰¹é‡é¢„æµ‹å¼‚å¸¸: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/model-metrics/<int:good_id>', methods=['GET'])
def model_metrics_endpoint(good_id):
    """è·å–æ¨¡å‹è´¨é‡æŒ‡æ ‡"""
    try:
        item_lock = CACHE_MANAGER.get_lock(good_id)
        with item_lock:
            model = CACHE_MANAGER.get(good_id)
            if model is None:
                model = PredictionModel(good_id)
                if not model.load_model():
                    return jsonify({'error': 'æ¨¡å‹ä¸å­˜åœ¨'}), 404
                CACHE_MANAGER.put(good_id, model)

            return jsonify({
                'good_id': good_id,
                'metrics': model.metrics,
                'weights': model.weights,
                'timestamp': datetime.now().isoformat()
            }), 200

    except Exception as e:
        logger.error(f"å¼‚å¸¸: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/alerts/<int:good_id>', methods=['GET'])
def get_alerts_endpoint(good_id):
    """è·å–æŸå•†å“çš„å‘Šè­¦ä¿¡æ¯"""
    try:
        active_alerts = ALERT_SYSTEM.get_active_alerts(good_id)
        return jsonify({
            'good_id': good_id,
            'total_alerts': len(active_alerts),
            'alerts': [asdict(a) for a in active_alerts],
            'timestamp': datetime.now().isoformat()
        }), 200
    except Exception as e:
        logger.error(f"è·å–å‘Šè­¦å¼‚å¸¸: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/alerts/active', methods=['GET'])
def get_active_alerts_endpoint():
    """è·å–æ‰€æœ‰æœªç¡®è®¤çš„å‘Šè­¦"""
    try:
        active_alerts = ALERT_SYSTEM.get_active_alerts()
        alert_summary = ALERT_SYSTEM.get_alert_summary()
        return jsonify({
            'summary': alert_summary,
            'timestamp': datetime.now().isoformat()
        }), 200
    except Exception as e:
        logger.error(f"è·å–æ´»è·ƒå‘Šè­¦å¼‚å¸¸: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/alerts/summary', methods=['GET'])
def alerts_summary_endpoint():
    """è·å–å‘Šè­¦æ‘˜è¦"""
    try:
        summary = ALERT_SYSTEM.get_alert_summary()
        return jsonify(summary), 200
    except Exception as e:
        logger.error(f"è·å–å‘Šè­¦æ‘˜è¦å¼‚å¸¸: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/data-quality/<int:good_id>', methods=['GET'])
def data_quality_endpoint(good_id):
    """è·å–æŸå•†å“çš„æ•°æ®è´¨é‡æŠ¥å‘Š"""
    try:
        item_lock = CACHE_MANAGER.get_lock(good_id)
        with item_lock:
            model = CACHE_MANAGER.get(good_id)
            if model is None:
                model = PredictionModel(good_id)
                if not model.load_model():
                    return jsonify({'error': 'æ¨¡å‹ä¸å­˜åœ¨æˆ–æ— è´¨é‡æŠ¥å‘Š'}), 404
                CACHE_MANAGER.put(good_id, model)

            quality_report = model.metrics.get('quality_report')
            drift_report = model.metrics.get('drift_report')

            if not quality_report or not drift_report:
                return jsonify({'error': 'å°šæ— è´¨é‡å’Œæ¼‚ç§»æŠ¥å‘Š'}), 404

            return jsonify({
                'good_id': good_id,
                'quality_report': quality_report,
                'drift_report': drift_report,
                'timestamp': datetime.now().isoformat()
            }), 200

    except Exception as e:
        logger.error(f"è·å–æ•°æ®è´¨é‡æŠ¥å‘Šå¼‚å¸¸: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clear-cache', methods=['POST'])
def clear_cache_endpoint():
    """æ¸…ç©ºæ¨¡å‹ç¼“å­˜"""
    try:
        size = CACHE_MANAGER.size()
        CACHE_MANAGER.clear()
        return jsonify({
            'status': 'success',
            'message': f'æ¸…ç©ºäº† {size} ä¸ªæ¨¡å‹ç¼“å­˜'
        }), 200
    except Exception as e:
        logger.error(f"æ¸…ç©ºç¼“å­˜å¼‚å¸¸: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/cache-status', methods=['GET'])
def cache_status_endpoint():
    """æŸ¥çœ‹ç¼“å­˜çŠ¶æ€"""
    try:
        status = {
            'cached_models': CACHE_MANAGER.size(),
            'max_size': CACHE_MANAGER.max_size,
            'model_dir': str(MODEL_DIR),
            'metrics_dir': str(METRICS_DIR),
            'timestamp': datetime.now().isoformat()
        }
        return jsonify(status), 200
    except Exception as e:
        logger.error(f"æŸ¥çœ‹ç¼“å­˜çŠ¶æ€å¼‚å¸¸: {e}")
        return jsonify({'error': str(e)}), 500


# ============================================================================
# å¯åŠ¨
# ============================================================================

if __name__ == '__main__':
    logger.info("=" * 60)
    logger.info("CSGO é¢„æµ‹æœåŠ¡ v2.1 (æ€§èƒ½ä¼˜åŒ–ç‰ˆ)")
    logger.info("ç‰¹æ€§: å¹¶å‘ä¼˜åŒ– | åŠ¨æ€æƒé‡ | äº¤æ˜“æ‰‹ç»­è´¹=1%")
    logger.info("=" * 60)
    logger.info(f"æ•°æ®åº“: {DB_CONFIG['host']}")
    logger.info(f"æ¨¡å‹ç›®å½•: {MODEL_DIR}")
    logger.info(f"æŒ‡æ ‡ç›®å½•: {METRICS_DIR}")
    logger.info("=" * 60)

    app.run(debug=False, host='0.0.0.0', port=5000, threaded=True)
